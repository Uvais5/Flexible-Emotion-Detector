
  {
   "cell_type": "markdown",
   "id": "46dec84d",
   "metadata": {},
   "source": [
    "We import everything needed for building and training a CNN:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|---------|\n",
    "| `tensorflow / keras` | Deep-learning backend (layers, model API, optimizers) |\n",
    "| `ImageDataGenerator` | Real-time image loading & on-the-fly preprocessing |\n",
    "| `numpy` | Numeric operations |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f518a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import  Conv2D, MaxPooling2D,Dense,Dropout,Flatten\n",
    "\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "from keras.models import Sequential\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f3c04",
   "metadata": {},
   "source": [
    "## üîÑ Preprocessing with `ImageDataGenerator`\n",
    "\n",
    "We use Keras‚Äô `ImageDataGenerator` to:\n",
    "\n",
    "- Rescale pixel values from **0‚Äì255 ‚Üí 0‚Äì1**\n",
    "- Load images in **batches of 64**\n",
    "- Automatically assign **one-hot encoded labels** from folder names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca85e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b82d19",
   "metadata": {},
   "source": [
    "### üîÑ Image Loading & Pre-processing with `flow_from_directory()`\n",
    "\n",
    "The two calls below create **data generators** that stream images from disk, resize\n",
    "them, scale their pixel values, and assign labels automatically.\n",
    "\n",
    "\n",
    "Each subfolder contains **face images labeled by emotion**.\n",
    "\n",
    "### üìä Dataset Statistics\n",
    "\n",
    "- üë®‚Äçüè´ Training images: `28,709`\n",
    "- üß™ Test images: `7,178`\n",
    "- üîñ Total classes: `7`\n",
    "\n",
    "\n",
    "    - üò† Angry  \n",
    "    - ü§¢ Disgust  \n",
    "    - üò® Fear  \n",
    "    - üòÑ Happy  \n",
    "    - üòê Neutral  \n",
    "    - üò¢ Sad  \n",
    "    - üò≤ Surprised\n",
    "- üñº Image format: Grayscale (1 channel), `48√ó48` pixels\n",
    "---\n",
    "\n",
    "\n",
    "We use Keras‚Äô `ImageDataGenerator` to:\n",
    "\n",
    "- Rescale pixel values from **0‚Äì255 ‚Üí 0‚Äì1**\n",
    "- Load images in **batches of 64**\n",
    "- Automatically assign **one-hot encoded labels** from folder names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d95e6c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "#preprocess all test images\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "   \"D:/face_emotion/train\",\n",
    "    target_size=(48,48),\n",
    "    batch_size=64,\n",
    "    color_mode = \"grayscale\",\n",
    "    class_mode = \"categorical\"\n",
    ")\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "    \"D:/face_emotion/test\",\n",
    "    target_size=(48,48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ee7a6",
   "metadata": {},
   "source": [
    "¬†# CNN Architecture¬†\n",
    "A classic VGG-style stack:\n",
    "\n",
    "1. **Conv(32) ‚Üí Conv(64) ‚Üí MaxPool ‚Üí Dropout**  \n",
    "2. **Conv(128) ‚Üí MaxPool ‚Üí Conv(128) ‚Üí MaxPool ‚Üí Dropout**  \n",
    "3. **Flatten ‚Üí Dense(1024) ‚Üí Dropout ‚Üí Dense(7-softmax)**  \n",
    "\n",
    "*Dropout* combats overfitting; **softmax** outputs class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614a38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64,kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=(3,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128,kernel_size=(3,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\",metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7047cb",
   "metadata": {},
   "source": [
    "# Training\n",
    "After 25 epochs we achieve 72 % train / 61 % val accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3170cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "448/448 [==============================] - 249s 546ms/step - loss: 1.7447 - accuracy: 0.2880 - val_loss: 1.5507 - val_accuracy: 0.3983\n",
      "Epoch 2/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 1.4870 - accuracy: 0.4254 - val_loss: 1.3643 - val_accuracy: 0.4763\n",
      "Epoch 3/25\n",
      "448/448 [==============================] - 12s 26ms/step - loss: 1.3522 - accuracy: 0.4772 - val_loss: 1.2593 - val_accuracy: 0.5151\n",
      "Epoch 4/25\n",
      "448/448 [==============================] - 12s 28ms/step - loss: 1.2747 - accuracy: 0.5120 - val_loss: 1.2335 - val_accuracy: 0.5289\n",
      "Epoch 5/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 1.2232 - accuracy: 0.5311 - val_loss: 1.1805 - val_accuracy: 0.5472\n",
      "Epoch 6/25\n",
      "448/448 [==============================] - 13s 28ms/step - loss: 1.1801 - accuracy: 0.5506 - val_loss: 1.1361 - val_accuracy: 0.5642\n",
      "Epoch 7/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 1.1488 - accuracy: 0.5621 - val_loss: 1.1374 - val_accuracy: 0.5649\n",
      "Epoch 8/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 1.1161 - accuracy: 0.5756 - val_loss: 1.1076 - val_accuracy: 0.5755\n",
      "Epoch 9/25\n",
      "448/448 [==============================] - 12s 28ms/step - loss: 1.0846 - accuracy: 0.5915 - val_loss: 1.1007 - val_accuracy: 0.5859\n",
      "Epoch 10/25\n",
      "448/448 [==============================] - 12s 28ms/step - loss: 1.0638 - accuracy: 0.5945 - val_loss: 1.0830 - val_accuracy: 0.5904\n",
      "Epoch 11/25\n",
      "448/448 [==============================] - 13s 28ms/step - loss: 1.0362 - accuracy: 0.6068 - val_loss: 1.0924 - val_accuracy: 0.5894\n",
      "Epoch 12/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 1.0131 - accuracy: 0.6180 - val_loss: 1.0729 - val_accuracy: 0.5949\n",
      "Epoch 13/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.9854 - accuracy: 0.6284 - val_loss: 1.0684 - val_accuracy: 0.6003\n",
      "Epoch 14/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.9658 - accuracy: 0.6373 - val_loss: 1.0623 - val_accuracy: 0.6046\n",
      "Epoch 15/25\n",
      "448/448 [==============================] - 13s 28ms/step - loss: 0.9364 - accuracy: 0.6460 - val_loss: 1.0713 - val_accuracy: 0.6052\n",
      "Epoch 16/25\n",
      "448/448 [==============================] - 13s 29ms/step - loss: 0.9157 - accuracy: 0.6544 - val_loss: 1.0637 - val_accuracy: 0.6109\n",
      "Epoch 17/25\n",
      "448/448 [==============================] - 13s 28ms/step - loss: 0.8942 - accuracy: 0.6640 - val_loss: 1.0543 - val_accuracy: 0.6105\n",
      "Epoch 18/25\n",
      "448/448 [==============================] - 12s 28ms/step - loss: 0.8718 - accuracy: 0.6707 - val_loss: 1.0736 - val_accuracy: 0.6049\n",
      "Epoch 19/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.8485 - accuracy: 0.6814 - val_loss: 1.0676 - val_accuracy: 0.6095\n",
      "Epoch 20/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.8341 - accuracy: 0.6861 - val_loss: 1.0697 - val_accuracy: 0.6151\n",
      "Epoch 21/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.8095 - accuracy: 0.6961 - val_loss: 1.0589 - val_accuracy: 0.6137\n",
      "Epoch 22/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.7933 - accuracy: 0.6991 - val_loss: 1.0660 - val_accuracy: 0.6127\n",
      "Epoch 23/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.7706 - accuracy: 0.7098 - val_loss: 1.0860 - val_accuracy: 0.6175\n",
      "Epoch 24/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.7528 - accuracy: 0.7202 - val_loss: 1.0966 - val_accuracy: 0.6148\n",
      "Epoch 25/25\n",
      "448/448 [==============================] - 12s 27ms/step - loss: 0.7367 - accuracy: 0.7226 - val_loss: 1.1091 - val_accuracy: 0.6109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20475244490>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_generator,steps_per_epoch=28709//64, epochs=25,validation_data=validation_generator,validation_steps=7178//64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535c331",
   "metadata": {},
   "source": [
    "Three formats for future use:\n",
    "\n",
    "| File | Why save it? |\n",
    "|------|--------------|\n",
    "| `Face_emotion_model_js.json` | Architecture only (lightweight) |\n",
    "| `Face_emotion_model_js.h5`  | Weights only (for quick reload) |\n",
    "| `Face_emotion_model.h5`     | Full model (architecture + weights) |\n",
    "\n",
    "These go into the `saved_model/` folder so the Streamlit app can load them at inference time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "793a11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"saved_model/Face_emotion_model_js.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb1a3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save trained model weight in .h5 file\n",
    "model.save_weights('saved_model/Face_emotion_model_js.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebd2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save(\"saved_model/Face_emotion_model.h5\")\n",
    "loaded_model = load_model(\"saved_model/Face_emotion_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0e6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
